import requests #API
import sqlite3 #db
import time 
from datetime import datetime
from bs4 import BeautifulSoup #html parser
import spacy #NLP
import unicodedata #module that allows Unicode char manipulation 
import pandas as pd
import math
from spacy.lang.en.stop_words import STOP_WORDS #its gonna help in stage 2. to filter stopwords, not having to use doc.
import pickle
import numpy as nd
from scipy import sparse
import gc


from sklearn.feature_extraction.text import TfidfVectorizer # for the machine learning later in this code
from sklearn.linear_model import LogisticRegression ## for the machine learning later in this code
from sklearn.model_selection import train_test_split ## for the machine learning later in this code
from sklearn.metrics import classification_report ## for the machine learning later in this code

from sentence_transformers import SentenceTransformer ##gonna be used for creating the embeddings (vector with 768 dims)

from sklearn.cluster import KMeans ## gonna be used for clustering
import os


#os 3 estagios est√£o muito bons, apesar de ter lixo ainda, mas tamos indo muito bem, at√© validei uma amostragem de 1k com o gpt
# conseguimos separar o lixo de "somos uma empresa assim assim..." e ficar com o que importa. At√© o lixo aqui √© proximo das skills.


LOAD_DIR = "C:\\Users\\ygorg\\OneDrive\\Documentos\\Pickles for remotive project"
path = os.path.join(LOAD_DIR, "stage3_data.pkl")


with open (path, "rb") as d:
    stage3_data = pickle.load(d)


bloco_de_descricao = stage3_data["bloco_de_descricao"]
positives_stage3_shorter = stage3_data["positives_stage3_shorter"]
vectorizer = stage3_data["vectorizer"]
model = stage3_data["model"]     


df5 = pd.DataFrame(positives_stage3_shorter[:1000000])
df5.to_excel("testing positive short grams.xlsx", index= False)

print ("excel para teste do positive short grams gerado")

# testei o positive stage3_shorter e ele tem 1-3 grams. tudo certo

#PROXIMOS PASSOS:

##üìå Etapas seguintes (bem simples):

##1) Gerar embeddings dos positivos
##
##Transformar cada token (unigram/bigram/trigram) em um vetor ‚Äúque entende o significado‚Äù.
##‚Üí Isso permite separar skills reais de palavras gen√©ricas.
##
##Exemplo:
##
##‚Äúpython‚Äù, ‚Äúsql‚Äù, ‚Äúdocker‚Äù ‚Üí ficam pr√≥ximos.
##
##‚Äúcreate‚Äù, ‚Äúmaintain‚Äù, ‚Äúlearning‚Äù ‚Üí ficam longe.
## O embedding gera um vetor que tem DIVERSAS dimens√µes. √â como se ele tivesse colcoando cada palavra nesse espa√ßo multidimensional de
##768 dimens√µes. Palavras semelhantes ter√£o esses vetores pr√≥ximos. enquanto muito diferentes, distantes. Assim ele mede a proximidade
## de significado.
## cada dimens√£o √© como uma coordenada abstrata, que ele usa para "entender o significado" da palavra.
## O modelo j√° √© treinado com milh√µes de frases da internet, livros etc. Ele sabe identificar inclusive palavras que tem mais de um 
## significado tipo "manga". Ele olha o contexto na frase. Ou seja √© s√≥ aplicar o modelo na palavra ou frase para gerar os embeddings.
## pelo visto aqui vai ser o "sentence-transformers"
## Ou seja os embeddings tamb√©m s√£o vetores, mas √© diferente do tf-idf. Enquanto o tfidf √© calculado para medir o peso das palavras,
## com base em ocorrencia no documento*ocorrencia em todos os documentos. Em que um √© o oposto do outro nessa equa√ß√£o; Os vetores do
## embeddings s√£o calculados de acordo com o que o modelo j√° foi pr√©-treinado. N√£o existe ‚Äúocorr√™ncia no documento‚Äù nessa parte; o 
# modelo j√° sabe, pela sua experi√™ncia pr√©via, que ‚ÄúPython‚Äù e ‚ÄúSQL‚Äù s√£o conceitos relacionados, mesmo que apare√ßam poucas vezes no seu dataset.

model_embeddings = SentenceTransformer('all-MiniLM-L6-v2') ## √© o modelo 'all-MiniLM-L6-v2' que √© leve, r√°pido e bom para tarefas de similaridade sem√¢ntica
#ele que gera os embeddings de palavras. all-MiniLM √© o modelo, "L6" significa que ele tem 6 camadas de aten√ß√£o. E v2 √© a vers√£o dele.
#'all' indica que foi treinado para gerar embeddings para senten√ßas e palavras em geral nao s√≥ tarefas especificas.

all_grams = []

for jobid,grams in positives_stage3_shorter:
    all_grams.append(grams)

##all_grams √© uma lista de listas, em que cada lista √© uma gram

grams_as_strings = [" ".join (g).strip() for g in all_grams if len(" ".join(g).strip())>0]

unique_grams = sorted(set(grams_as_strings)) #retuns 557k unique grams
#####temos um problema aqui. A minha lista de unique_grams tem 577k de registros. E segundo o GPT
#####√© muito pesado gerar os embeddings para tudo isso. O problema n√£o √© nem os clusters, e sim
##### os embeddings.
#### vou ter que dar um jeito de filtrar.

#### ESTRATEGIA possivel, embedar 150k e treinar o kmeans com esses 150k e depois embedar o restante
#### em batches e passar pelo k-means. Segundo o GPT isso vai ser leve.

sample_size = 150000

first150k_unique_grams = unique_grams[:sample_size]
first150k_embeddings = model_embeddings.encode(first150k_unique_grams,batch_size=512, convert_to_numpy=True)

k =20

kmeans = KMeans(n_clusters=k,random_state=42,n_init='auto')
kmeans.fit(first150k_embeddings)

cluster_dict = {}

first150k_labels = kmeans.predict(first150k_embeddings)
for gram, cluster_id in zip (first150k_unique_grams,first150k_labels):
    cluster_dict[gram] = int(cluster_id)

batch_size = 500
remaining_grams = unique_grams[sample_size:]

for i in range (0,len(remaining_grams),batch_size):
    batch = remaining_grams[i:i+batch_size]

    batch_embeddings = model_embeddings.encode(batch,batch_size=512,convert_to_numpy=True)

    batch_labels = kmeans.predict(batch_embeddings)

    for gram, cluster_id in zip(batch,batch_labels):
        cluster_dict[gram] = int(cluster_id)

print (cluster_dict)

clusterdict_inlist_to_slice_then_dictagain = dict(list(cluster_dict.items())[:500000])

df10 = pd.DataFrame([{"gram": gram, "cluster_id": cluster_id} for gram,cluster_id in 
clusterdict_inlist_to_slice_then_dictagain.items()])
df10.to_excel("clusterIDtocheck.xlsx",index=False)


print ("cluster dict conclu√≠do")

###onde estou agora: decidir se vamos pegar aquela parte de actividades que descreve o que a pessoa
### vai fazer no dia a dia tamb√©m, ou s√≥ pegar os requirements/ skills/ must have.

### n√£o daria muito trabalho provavelmente, talvez fosse s√≥ uma quest√£o de no stage 1
### cortar tudo que vem antes de "requirements", "skills", "what we're looking for"...

## o GPT me aconselhou a pegar s√≥ as skills e excluir as atividades. Pq se n√£o o gr√°fico vai 
## virar uma zona. To inclinado a isso tamb√©m.

## outro ponto √©, escolher os clusters depois. talvez o melhor seja dar um passo atras e s√≥ trabalhar
## com o texto que vem antes desses pontos. O que vai ajudar demais na hora incluso de escolher os 
## clusters.

## ---------------------------------------------------------------------------------------------------


## A ESTRAT√âGIA PARA N√ÉO DAR PAU NA HORA DE CORTAR DE positives_stage3_shorter (PARA MANTERMOS O JOB ID) vai ser, depois do clustering,
##usar um dicionario, para a opera√ß√£o ser 0(1). Ent√£o depois do clustering vamos fazer algo assim: cluster_dict = {gram: cluster_id,....}
#e bater esse dict com a positives_stage3_shorter


##embeddings = model.encode(unique_grams) ##passando as 1-3 grams que sobraram na ML do estagio anterior para gerar os embeddings.
#a fun√ß√£o encode pega cada string e transforma em um vetor de numeros



##k = 20

## explica√ß√£o sobre os clusters que o kmeans vai fazer.
## clustering √© olhar para as 768 ou 384 dimensoes que foram atribuidas como vetores a cada gram, e vai AGRUPAR POR SEMELHAN√áA.
## de acordo com o que essas dimens√µes mostram de cada palavra. como tem um MONTE, ele consegue olhar e entender o significado dessas palavras
## de acordo com a proximidade entre essas dimens√µes.
## O kmeans vai as grams e atribuir um cluster para cada gram. √â como se tivesse uma multid√£o de pessoas, e ele falasse "idosos na fila 1"
## "crian√ßas na fila 2", "programadores fila 3"... etc. est√° agrupando por semelhan√ßa;


##kmeans = KMeans(n_clusters=k,random_state=42, n_init='auto') 
##
##labels = kmeans.fit_predict(embeddings)
##
##cluster_dict = {gram: int(cluster_id) for gram, cluster_id in zip(unique_grams, labels)}
##
##print (cluster_dict)

#### REALMENTE, HA ALGUMAS PALAVRAS QUE EST√ÉO VINDO J√Å COLADAS (TALVEZ DO PROPRIO JSON OU DO TRATAMENTO DO BS4.)
### E S√ÉO POUCAS, UMAS 5K DOS 1MM DO BLOCO DESCRICAO. ACONTECE QUE POR SUA RARIDADE O TFIDF DELA DA ALTO E ELA PASSA COMO POSITIVA PELO ML.
### E O CLUSTER TAMB√âM COSTUMA ESCOLHER ESSAS PALAVRAS PARA EMCABE√áAR A FILA. POR ISSO ESTAMOS VENDO ELAS MUITO NOS CLUSTERS.
### AGORA TEM QUE VER SE N√ÉO √â VIAJEM DO GPT ISSO.


##obs tem umas grams que vieram com palavras coladas, provavelmente na hora de separar os /z etc, n√£o colocarmos espa√ßo.
## o gpt falou que mesmo assim ele identifica, mas precisamos averiguar isso.









##2) Fazer clustering nos embeddings
##
##Juntar vetores parecidos.
##Use HDBSCAN (melhor) ou K-means (mais simples).
##
##Isso vai formar grupos como:
##
##Cluster 1 ‚Üí python / sql / tableau / power bi
##
##Cluster 2 ‚Üí create / maintain / learning / assessment
##
##Cluster 3 ‚Üí articular / captivate / rise
##
##3) Identificar quais clusters representam SKILLS
##
##Voc√™ olha apenas os top tokens de cada cluster.
##Se for skill ‚Üí voc√™ marca como ‚Äúskill cluster‚Äù.
##Se for verbo gen√©rico ‚Üí ignora.
##
##√â r√°pido e totalmente manual, mas √© s√≥ olhar 20 clusters, n√£o 6 milh√µes de tokens.
##
##4) Gerar seu dicion√°rio final de skills
##
##Pegue todos tokens nos clusters marcados como SKILLS.
##Pronto: voc√™ criou seu dicion√°rio final.
##
##5) Filtrar os positivos
##
##Agora √© f√°cil:
##S√≥ manter tokens cujo cluster = ‚Äúskill‚Äù.
##
##Isso reduz de milh√µes para s√≥ as skills reais.
